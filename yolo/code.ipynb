{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv7 Code Review\n",
    "- 데이터 로드 > 학습 > 테스트 > 탐지를 프로세스로 지정하고, 각각에 대응되는 소스코드를 탐색합니다.\n",
    "- `train.py`에서 데이터를 불러오는 부분을 중심으로 전체적인 데이터 처리 과정을 탐색합니다.\n",
    "- `train.py`와 `models/yolo.py`를 중심으로 전체적인 학습 과정을 탐색합니다.\n",
    "- `test.py`를 중심으로 전체적인 테스트 과정을 탐색합니다.\n",
    "- `detect.py`를 중심으로 전체적인 객체 인식 과정을 탐색합니다.\n",
    "- Computer Vision에 대한 깊은 이해가 없음을 전제로 피상적인 수준에서의 간단한 리뷰를 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import yaml\n",
    "\n",
    "def train(hyp, opt, device, tb_writer=None):\n",
    "    # ...\n",
    "    with open(opt.data) as f:\n",
    "        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  # data dict\n",
    "    is_coco = opt.data.endswith('coco.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train.py` 내에서 데이터를 불러오는 것과 관련된 부분만 탐색했습니다.\n",
    "- `train()` 함수에서 yaml 파일을 읽어오는 것부터 데이터 처리가 수행되는데,   \n",
    "  해당하는 yaml 파일로는 기본으로 존재하는 아래 `data/coco.yaml` 파일을 참고해 분석합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# COCO 2017 dataset http://cocodataset.org\n",
    "\n",
    "# download command/URL (optional)\n",
    "download: bash ./scripts/get_coco.sh\n",
    "\n",
    "# train and val data as 1) directory: path/images/, 2) file: path/images.txt, or 3) list: [path1/images/, path2/images/]\n",
    "train: ./coco/train2017.txt  # 118287 images\n",
    "val: ./coco/val2017.txt  # 5000 images\n",
    "test: ./coco/test-dev2017.txt  # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794\n",
    "\n",
    "# number of classes\n",
    "nc: 80\n",
    "\n",
    "# class names\n",
    "names: [ 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "         'hair drier', 'toothbrush' ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLOv7 저장소 내부엔 따로 데이터 파일이 존재하지 않는데 쉘 스크립트를 사용하여,   \n",
    "  YOLOv5 저장소로부터 label을, coco 데이터셋의 train,val,test 압축파일을 다운받고 해체하도록 설계되었습니다.\n",
    "- 파일명과 같은 이미지 파일에 대한 정보가 담긴 텍스트 파일을 참조해 이미지 데이터를 불러오는데,   \n",
    "  커스텀 데이터셋을 사용할 경우에도 유사한 파일 형식 및 구조를 적용하면 학습 데이터셋으로 사용할 수 있습니다.\n",
    "- 클래스 수와 각각의 클래스 별 명칭을 리스트로 전달함으로써,   \n",
    "  학습 시 디렉토리 단위로 클래스를 나누고, 테스트 시에 객체 예측 결과를 문자열로 변환해서 보여주도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import numpy as np\n",
    "from models.yolo import Model\n",
    "from utils.datasets import create_dataloader\n",
    "\n",
    "def train(hyp, opt, device, tb_writer=None):\n",
    "    # ...\n",
    "    with open(opt.data) as f:\n",
    "        data_dict = yaml.load(f, Loader=yaml.SafeLoader)  # data dict\n",
    "    is_coco = opt.data.endswith('coco.yaml')\n",
    "    # ...\n",
    "    nc = 1 if opt.single_cls else int(data_dict['nc'])  # number of classes\n",
    "    names = ['item'] if opt.single_cls and len(data_dict['names']) != 1 else data_dict['names']  # class names\n",
    "    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  # check\n",
    "    # ...\n",
    "    pretrained = weights.endswith('.pt')\n",
    "    if pretrained:\n",
    "        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "    # ...\n",
    "    train_path = data_dict['train']\n",
    "    test_path = data_dict['val']\n",
    "    # ...\n",
    "    gs = max(int(model.stride.max()), 32)  # grid size (max stride)\n",
    "    nl = model.model[-1].nl  # number of detection layers (used for scaling hyp['obj'])\n",
    "    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n",
    "    # ...\n",
    "    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt,\n",
    "                                            hyp=hyp, augment=True, cache=opt.cache_images, rect=opt.rect, rank=rank,\n",
    "                                            world_size=opt.world_size, workers=opt.workers,\n",
    "                                            image_weights=opt.image_weights, quad=opt.quad, prefix=colorstr('train: '))\n",
    "    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n",
    "    nb = len(dataloader)  # number of batches\n",
    "    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- yaml 파일에서 읽어온 클래스 수에 대한 값과 하이퍼 파라미터를 인자로 모델을 생성하고,   \n",
    "  이미지 크기가 모델의 그리드 크기와 동일한 비율인지 검증하는 작업을 거칩니다.\n",
    "- 파일 경로 및 기타 파라미터들을 `create_dataloader()`에 전달하여 dataloader를 생성하는데,   \n",
    "  해당하는 함수의 기능에 대해 알아봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/datasets.py\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=None, augment=False, cache=False, pad=0.0, rect=False,\n",
    "                      rank=-1, world_size=1, workers=8, image_weights=False, quad=False, prefix=''):\n",
    "    # Make sure only the first process in DDP process the dataset first, and the following others can use the cache\n",
    "    with torch_distributed_zero_first(rank):\n",
    "        dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
    "                                      augment=augment,  # augment images\n",
    "                                      hyp=hyp,  # augmentation hyperparameters\n",
    "                                      rect=rect,  # rectangular training\n",
    "                                      cache_images=cache,\n",
    "                                      single_cls=opt.single_cls,\n",
    "                                      stride=int(stride),\n",
    "                                      pad=pad,\n",
    "                                      image_weights=image_weights,\n",
    "                                      prefix=prefix)\n",
    "\n",
    "    batch_size = min(batch_size, len(dataset))\n",
    "    nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, workers])  # number of workers\n",
    "    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None\n",
    "    loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader\n",
    "    # Use torch.utils.data.DataLoader() if dataset.properties will update during training else InfiniteDataLoader()\n",
    "    dataloader = loader(dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        num_workers=nw,\n",
    "                        sampler=sampler,\n",
    "                        pin_memory=True,\n",
    "                        collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn)\n",
    "    return dataloader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImagesAndLabels(Dataset):  # for training/testing\n",
    "    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n",
    "                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):\n",
    "        # ...\n",
    "        try:\n",
    "            f = []  # image files\n",
    "            for p in path if isinstance(path, list) else [path]:\n",
    "                p = Path(p)  # os-agnostic\n",
    "                if p.is_dir():  # dir\n",
    "                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
    "                    # f = list(p.rglob('**/*.*'))  # pathlib\n",
    "                elif p.is_file():  # file\n",
    "                    with open(p, 'r') as t:\n",
    "                        t = t.read().strip().splitlines()\n",
    "                        parent = str(p.parent) + os.sep\n",
    "                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n",
    "                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
    "                else:\n",
    "                    raise Exception(f'{prefix}{p} does not exist')\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{prefix}Error loading data from {path}: {e}\\nSee {help_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `create_dataloader()`는 우선 `LoadImagesAndLabels`라는 데이터셋을 생성하는데,   \n",
    "  해당 객체는 위 코드에서 확인할 수 있듯이 PyTorch Dataset 구조를 상속받으며,   \n",
    "  객체 생성 시 전달받은 `train_path`를 기준으로 하위 디렉토리 내 파일들을 모두 읽어옵니다.\n",
    "- 데이터셋은 batch size와 같은 하이퍼 파라미터와 같이 PyTorch Dataloader 객체를 생성하는데 전달되며,   \n",
    "  생성된 Dataloader와 Dataset을 실행 결과로 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parsing arguments\n",
    "    opt = parser.parse_args()\n",
    "    # wandb settings\n",
    "    # DDP mode\n",
    "\n",
    "    with open(opt.hyp) as f:\n",
    "        hyp = yaml.load(f, Loader=yaml.SafeLoader)  # load hyps\n",
    "\n",
    "    logger.info(opt)\n",
    "    if not opt.evolve:\n",
    "        tb_writer = None  # init loggers\n",
    "        if opt.global_rank in [-1, 0]:\n",
    "            prefix = colorstr('tensorboard: ')\n",
    "            logger.info(f\"{prefix}Start with 'tensorboard --logdir {opt.project}', view at http://localhost:6006/\")\n",
    "            tb_writer = SummaryWriter(opt.save_dir)  # Tensorboard\n",
    "        train(hyp, opt, device, tb_writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train.py`를 실행했을 때 가장 먼저 실행되는 main 함수에서,   \n",
    "  우선적으로 argument 추출, wandb 연결, DDP 모드 등을 설정하고,   \n",
    "  지정된 yaml 파일로부터 하이퍼 파라미터를 읽어와 `train()` 함수의 매개변수로 전달하며 학습을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "from models.yolo import Model\n",
    "\n",
    "def train(hyp, opt, device, tb_writer=None):\n",
    "    logger.info(colorstr('hyperparameters: ') + ', '.join(f'{k}={v}' for k, v in hyp.items()))\n",
    "    save_dir, epochs, batch_size, total_batch_size, weights, rank, freeze = \\\n",
    "        Path(opt.save_dir), opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.global_rank, opt.freeze\n",
    "    # ...\n",
    "    pretrained = weights.endswith('.pt')\n",
    "    if pretrained:\n",
    "        with torch_distributed_zero_first(rank):\n",
    "            attempt_download(weights)  # download if not found locally\n",
    "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
    "        model = Model(opt.cfg or ckpt['model'].yaml, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "        exclude = ['anchor'] if (opt.cfg or hyp.get('anchors')) and not opt.resume else []  # exclude keys\n",
    "        state_dict = ckpt['model'].float().state_dict()  # to FP32\n",
    "        state_dict = intersect_dicts(state_dict, model.state_dict(), exclude=exclude)  # intersect\n",
    "        model.load_state_dict(state_dict, strict=False)  # load\n",
    "        logger.info('Transferred %g/%g items from %s' % (len(state_dict), len(model.state_dict()), weights))  # report\n",
    "    else:\n",
    "        model = Model(opt.cfg, ch=3, nc=nc, anchors=hyp.get('anchors')).to(device)  # create\n",
    "    with torch_distributed_zero_first(rank):\n",
    "        check_dataset(data_dict)  # check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train.py`를 실행할 때 pt 파일의 형식으로 pre-trained weight를 전달할 경우,   \n",
    "  해당하는 파일로부터 가중치를 읽어와 모델을 초기화하고, 그렇지 않을 경우 전달된 가중치를 직접 적용합니다.\n",
    "- 모델의 세부 구조를 파악하기 위해 `models/yolo.py`로부터 모델 클래스를 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/yolo.py\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, cfg='yolor-csp-c.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\n",
    "        super(Model, self).__init__()\n",
    "        self.traced = False\n",
    "        if isinstance(cfg, dict):\n",
    "            self.yaml = cfg  # model dict\n",
    "        else:  # is *.yaml\n",
    "            import yaml  # for torch hub\n",
    "            self.yaml_file = Path(cfg).name\n",
    "            with open(cfg) as f:\n",
    "                self.yaml = yaml.load(f, Loader=yaml.SafeLoader)  # model dict\n",
    "\n",
    "        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\n",
    "        if nc and nc != self.yaml['nc']:\n",
    "            logger.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n",
    "            self.yaml['nc'] = nc  # override yaml value\n",
    "        if anchors:\n",
    "            logger.info(f'Overriding model.yaml anchors with anchors={anchors}')\n",
    "            self.yaml['anchors'] = round(anchors)  # override yaml value\n",
    "        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n",
    "        self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\n",
    "\n",
    "        # Build strides, anchors\n",
    "        m = self.model[-1]  # Detect()\n",
    "        if isinstance(m, Detect):\n",
    "            s = 256  # 2x min stride\n",
    "            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n",
    "            check_anchor_order(m)\n",
    "            m.anchors /= m.stride.view(-1, 1, 1)\n",
    "            self.stride = m.stride\n",
    "            self._initialize_biases()  # only run once\n",
    "            # print('Strides: %s' % m.stride.tolist())\n",
    "        # ...\n",
    "        initialize_weights(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Model()`은 초기 생성 시 전달받은 yaml 파일로부터 channel, anchor와 같은 정보를 읽어서   \n",
    "  `parse_model()`이라는 함수에 전달해 모델 구조를 생성합니다.\n",
    "- 이후 모델의 타입에 따라 downsampling을 위한 stride 및 anchor를 생성하고, weight를 초기화 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models/yolo.py\n",
    "\n",
    "def parse_model(d, ch):  # model_dict, input_channels(3)\n",
    "    logger.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n",
    "    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n",
    "    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n",
    "    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n",
    "\n",
    "    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n",
    "    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n",
    "        m = eval(m) if isinstance(m, str) else m  # eval strings\n",
    "        for j, a in enumerate(args):\n",
    "            try:\n",
    "                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n",
    "        if m in [nn.Conv2d, Conv, RobustConv, RobustConv2, DWConv, GhostConv, RepConv, RepConv_OREPA, DownC, \n",
    "                 SPP, SPPF, SPPCSPC, GhostSPPCSPC, MixConv2d, Focus, Stem, GhostStem, CrossConv, \n",
    "                 Bottleneck, BottleneckCSPA, BottleneckCSPB, BottleneckCSPC, \n",
    "                 RepBottleneck, RepBottleneckCSPA, RepBottleneckCSPB, RepBottleneckCSPC,  \n",
    "                 Res, ResCSPA, ResCSPB, ResCSPC, \n",
    "                 RepRes, RepResCSPA, RepResCSPB, RepResCSPC, \n",
    "                 ResX, ResXCSPA, ResXCSPB, ResXCSPC, \n",
    "                 RepResX, RepResXCSPA, RepResXCSPB, RepResXCSPC, \n",
    "                 Ghost, GhostCSPA, GhostCSPB, GhostCSPC,\n",
    "                 SwinTransformerBlock, STCSPA, STCSPB, STCSPC,\n",
    "                 SwinTransformer2Block, ST2CSPA, ST2CSPB, ST2CSPC]:\n",
    "            c1, c2 = ch[f], args[0]\n",
    "            if c2 != no:  # if not output\n",
    "                c2 = make_divisible(c2 * gw, 8)\n",
    "\n",
    "            args = [c1, c2, *args[1:]]\n",
    "            if m in [DownC, SPPCSPC, GhostSPPCSPC, \n",
    "                     BottleneckCSPA, BottleneckCSPB, BottleneckCSPC, \n",
    "                     RepBottleneckCSPA, RepBottleneckCSPB, RepBottleneckCSPC, \n",
    "                     ResCSPA, ResCSPB, ResCSPC, \n",
    "                     RepResCSPA, RepResCSPB, RepResCSPC, \n",
    "                     ResXCSPA, ResXCSPB, ResXCSPC, \n",
    "                     RepResXCSPA, RepResXCSPB, RepResXCSPC,\n",
    "                     GhostCSPA, GhostCSPB, GhostCSPC,\n",
    "                     STCSPA, STCSPB, STCSPC,\n",
    "                     ST2CSPA, ST2CSPB, ST2CSPC]:\n",
    "                args.insert(2, n)  # number of repeats\n",
    "                n = 1\n",
    "        # ...\n",
    "        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)  # module\n",
    "        t = str(m)[8:-2].replace('__main__.', '')  # module type\n",
    "        np = sum([x.numel() for x in m_.parameters()])  # number params\n",
    "        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n",
    "        logger.info('%3s%18s%3s%10.0f  %-40s%-30s' % (i, f, n, np, t, args))  # print\n",
    "        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n",
    "        layers.append(m_)\n",
    "        if i == 0:\n",
    "            ch = []\n",
    "        ch.append(c2)\n",
    "    return nn.Sequential(*layers), sorted(save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `parse_model()` 함수에서는 yaml 파일로부터 backbone과 head에 대한 리스트를 읽어와   \n",
    "  Sequential 구조를 생성합니다.\n",
    "- 모델 구조에 대한 yaml 파일에 대해 알아보기 위해 아래 `yolov7-tiny.yaml` 파일을 확인했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# parameters\n",
    "nc: 80  # number of classes\n",
    "depth_multiple: 1.0  # model depth multiple\n",
    "width_multiple: 1.0  # layer channel multiple\n",
    "\n",
    "# anchors\n",
    "anchors:\n",
    "  - [10,13, 16,30, 33,23]  # P3/8\n",
    "  - [30,61, 62,45, 59,119]  # P4/16\n",
    "  - [116,90, 156,198, 373,326]  # P5/32\n",
    "\n",
    "# yolov7-tiny backbone\n",
    "backbone:\n",
    "  # [from, number, module, args] c2, k=1, s=1, p=None, g=1, act=True\n",
    "  [[-1, 1, Conv, [32, 3, 2, None, 1, nn.LeakyReLU(0.1)]],  # 0-P1/2  \n",
    "   # ...\n",
    "  ]\n",
    "\n",
    "# yolov7-tiny head\n",
    "head:\n",
    "  [[-1, 1, Conv, [256, 1, 1, None, 1, nn.LeakyReLU(0.1)]],\n",
    "   # ...\n",
    "   [[74,75,76], 1, IDetect, [nc, anchors]],   # Detect(P3, P4, P5)\n",
    "  ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- YOLOv7 모델의 backbone과 head는 위와 같은 convolutional layer의 조합으로 이루어져 있으며,   \n",
    "  head의 마지막에 IDetect layer가 붙어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "\n",
    "from utils.loss import ComputeLoss, ComputeLossOTA\n",
    "\n",
    "def train(hyp, opt, device, tb_writer=None):\n",
    "    # ...\n",
    "    t0 = time.time()\n",
    "    nw = max(round(hyp['warmup_epochs'] * nb), 1000)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
    "    # nw = min(nw, (epochs - start_epoch) / 2 * nb)  # limit warmup to < 1/2 of training\n",
    "    maps = np.zeros(nc)  # mAP per class\n",
    "    results = (0, 0, 0, 0, 0, 0, 0)  # P, R, mAP@.5, mAP@.5-.95, val_loss(box, obj, cls)\n",
    "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
    "    scaler = amp.GradScaler(enabled=cuda)\n",
    "    compute_loss_ota = ComputeLossOTA(model)  # init loss class\n",
    "    compute_loss = ComputeLoss(model)  # init loss class\n",
    "    for epoch in range(start_epoch, epochs):  # epoch ------------------------------------------------------------------\n",
    "        model.train()\n",
    "        # ...\n",
    "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
    "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
    "            imgs = imgs.to(device, non_blocking=True).float() / 255.0  # uint8 to float32, 0-255 to 0.0-1.0\n",
    "\n",
    "            # Warmup\n",
    "            if ni <= nw:\n",
    "                xi = [0, nw]  # x interp\n",
    "                # model.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n",
    "                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\n",
    "                for j, x in enumerate(optimizer.param_groups):\n",
    "                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n",
    "                    x['lr'] = np.interp(ni, xi, [hyp['warmup_bias_lr'] if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
    "                    if 'momentum' in x:\n",
    "                        x['momentum'] = np.interp(ni, xi, [hyp['warmup_momentum'], hyp['momentum']])\n",
    "\n",
    "            # Multi-scale\n",
    "            if opt.multi_scale:\n",
    "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
    "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
    "                if sf != 1:\n",
    "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
    "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
    "\n",
    "            # Forward\n",
    "            with amp.autocast(enabled=cuda):\n",
    "                pred = model(imgs)  # forward\n",
    "                if 'loss_ota' not in hyp or hyp['loss_ota'] == 1:\n",
    "                    loss, loss_items = compute_loss_ota(pred, targets.to(device), imgs)  # loss scaled by batch_size\n",
    "                else:\n",
    "                    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
    "                if rank != -1:\n",
    "                    loss *= opt.world_size  # gradient averaged between devices in DDP mode\n",
    "                if opt.quad:\n",
    "                    loss *= 4.\n",
    "\n",
    "            # Backward\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            # Optimize\n",
    "            if ni % accumulate == 0:\n",
    "                scaler.step(optimizer)  # optimizer.step\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                if ema:\n",
    "                    ema.update(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다시 `train.py`로 돌아와 모델 생성 후 학습 과정을 살펴보았습니다.\n",
    "- 학습 과정은 정해진 epoch 동안 Dataloader에 저장된 batch 단위로 반복되며,   \n",
    "  YOLOv3부터 사용된 multi-scale을 수행하고, forward와 backward를 진행합니다.\n",
    "- 이 중에서 loss에 해당하는 부분만 따로 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/loss.py\n",
    "\n",
    "class ComputeLoss:\n",
    "    # Compute losses\n",
    "    def __init__(self, model, autobalance=False):\n",
    "        super(ComputeLoss, self).__init__()\n",
    "        device = next(model.parameters()).device  # get model device\n",
    "        h = model.hyp  # hyperparameters\n",
    "\n",
    "        # Define criteria\n",
    "        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))\n",
    "        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))\n",
    "\n",
    "        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3\n",
    "        self.cp, self.cn = smooth_BCE(eps=h.get('label_smoothing', 0.0))  # positive, negative BCE targets\n",
    "\n",
    "        # Focal loss\n",
    "        g = h['fl_gamma']  # focal loss gamma\n",
    "        if g > 0:\n",
    "            BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)\n",
    "\n",
    "        det = model.module.model[-1] if is_parallel(model) else model.model[-1]  # Detect() module\n",
    "        self.balance = {3: [4.0, 1.0, 0.4]}.get(det.nl, [4.0, 1.0, 0.25, 0.06, .02])  # P3-P7\n",
    "        #self.balance = {3: [4.0, 1.0, 0.4]}.get(det.nl, [4.0, 1.0, 0.25, 0.1, .05])  # P3-P7\n",
    "        #self.balance = {3: [4.0, 1.0, 0.4]}.get(det.nl, [4.0, 1.0, 0.5, 0.4, .1])  # P3-P7\n",
    "        self.ssi = list(det.stride).index(16) if autobalance else 0  # stride 16 index\n",
    "        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, model.gr, h, autobalance\n",
    "        for k in 'na', 'nc', 'nl', 'anchors':\n",
    "            setattr(self, k, getattr(det, k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ComputeLoss` loss function은 클래스와 object에 대한 cross entropy를 기반으로한   \n",
    "  Focal Loss를 사용합니다.\n",
    "- Focal Loss에 대해선 [해당 링크](https://gaussian37.github.io/dl-concept-focal_loss/)를 참조해 알아봤는데,   \n",
    "  one-stage detector에서 학습 중 클래스 불균형 문제를 해결하기 위한 목적의 알고리즘 입니다.\n",
    "- Cross Entropy Loss에 대한 식 $−Y_{act}\\log{(Y_{pred})}−(1−Y_{act})\\log(1−Y_{pred})$에서,  \n",
    "  잘 예측한 경우엔 패널티가 없어지는 반면, 잘 예측하지 못한 경우엔 패널티가 굉장히 커지게 됩니다.\n",
    "- Focal Loss의 경우 CE의 패널티 $-\\log{p_t}$에 $(1−p_t)^{\\gamma}$를 곱해 loss의 가중치를 줄입니다.\n",
    "- Object Detection 문제에서는 background에 대한 경우가 foreground 보다 훨씬 많기 때문에,   \n",
    "  Focal Loss를 사용하게 되는데, 실제로 과거 프로젝트에서 two-stage detector를 구현했던 경험에서,   \n",
    "  Cross Entropy Loss를 사용하여 만족스런 결과가 나오지 않았던 경험이 있습니다.\n",
    "- Focal Loss를 사용하게 된다면 확률이 높은 케이스에 상대적으로 loss를 크게 낮출 수 있는 보상을 주어   \n",
    "  background에 대한 수많은 loss 총합이 실제 객체 추정에 필요한 loss 총합보다 커지는 문제를 해결합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/loss.py\n",
    "\n",
    "class ComputeLoss:\n",
    "    def __call__(self, p, targets):  # predictions, targets, model\n",
    "        device = targets.device\n",
    "        lcls, lbox, lobj = torch.zeros(1, device=device), torch.zeros(1, device=device), torch.zeros(1, device=device)\n",
    "        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n",
    "\n",
    "        # Losses\n",
    "        for i, pi in enumerate(p):  # layer index, layer predictions\n",
    "            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx\n",
    "            tobj = torch.zeros_like(pi[..., 0], device=device)  # target obj\n",
    "\n",
    "            n = b.shape[0]  # number of targets\n",
    "            if n:\n",
    "                ps = pi[b, a, gj, gi]  # prediction subset corresponding to targets\n",
    "\n",
    "                # Regression\n",
    "                pxy = ps[:, :2].sigmoid() * 2. - 0.5\n",
    "                pwh = (ps[:, 2:4].sigmoid() * 2) ** 2 * anchors[i]\n",
    "                pbox = torch.cat((pxy, pwh), 1)  # predicted box\n",
    "                iou = bbox_iou(pbox.T, tbox[i], x1y1x2y2=False, CIoU=True)  # iou(prediction, target)\n",
    "                lbox += (1.0 - iou).mean()  # iou loss\n",
    "\n",
    "                # Objectness\n",
    "                tobj[b, a, gj, gi] = (1.0 - self.gr) + self.gr * iou.detach().clamp(0).type(tobj.dtype)  # iou ratio\n",
    "\n",
    "                # Classification\n",
    "                if self.nc > 1:  # cls loss (only if multiple classes)\n",
    "                    t = torch.full_like(ps[:, 5:], self.cn, device=device)  # targets\n",
    "                    t[range(n), tcls[i]] = self.cp\n",
    "                    #t[t==self.cp] = iou.detach().clamp(0).type(t.dtype)\n",
    "                    lcls += self.BCEcls(ps[:, 5:], t)  # BCE\n",
    "\n",
    "                # Append targets to text file\n",
    "                # with open('targets.txt', 'a') as file:\n",
    "                #     [file.write('%11.5g ' * 4 % tuple(x) + '\\n') for x in torch.cat((txy[i], twh[i]), 1)]\n",
    "\n",
    "            obji = self.BCEobj(pi[..., 4], tobj)\n",
    "            lobj += obji * self.balance[i]  # obj loss\n",
    "            if self.autobalance:\n",
    "                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()\n",
    "\n",
    "        if self.autobalance:\n",
    "            self.balance = [x / self.balance[self.ssi] for x in self.balance]\n",
    "        lbox *= self.hyp['box']\n",
    "        lobj *= self.hyp['obj']\n",
    "        lcls *= self.hyp['cls']\n",
    "        bs = tobj.shape[0]  # batch size\n",
    "\n",
    "        loss = lbox + lobj + lcls\n",
    "        return loss * bs, torch.cat((lbox, lobj, lcls, loss)).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss function을 사용하기 위해 `ComputeLoss`를 호출하는 경우에는,   \n",
    "  예측한 bbox와 실제 label과의 IoU를 계산하고, 동시에 classification에 대한 focal loss를 계산합니다.\n",
    "- 그리고 각각의 loss에 가중치를 곱한 값을 더해 하나의 loss로서 다룹니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Test\n",
    "- 테스트 과정부터는 다음에 기회가 된다면 추가로 다뤄보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('mldl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86ae205601b6d906014fa7892090616f7e1469eb0aa86f06d2d1803a695f1eb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
